{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e84db4",
   "metadata": {},
   "source": [
    "Peter Ling \n",
    "Logistic Regression Model\n",
    "\n",
    "\n",
    "In this project, I implemented a logistic regression algorithm to predict the output of a data point given its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "27c1eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "testing_file_location = \"./simple-test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54aaba",
   "metadata": {},
   "source": [
    "The sigmoid funciton is used as the squashing function in my algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efc4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return (1 / (1 + math.exp(-x)))\n",
    "\n",
    "def dot(a, b):\n",
    "\n",
    "    if len(a) != len(b):\n",
    "        return \"error\"\n",
    "\n",
    "    return sum(i[0] * i[1] for i in zip(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42ee2d",
   "metadata": {},
   "source": [
    "I used gradient ascent to train my parameters. This function takes in a dataframe of the training data, which will have parameters as the columns and the rows as each training data point. The last column will be the output of the training data point, which will come out to 0 or 1. It also takes in the number of training steps the user wants the gradient ascent to go through, along with the learning rate that would be appropritate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "635e4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent(training_df, training_steps, learning_rate):\n",
    "    \n",
    "    rows = len(training_df) \n",
    "    n_params = len(training_df.columns) - 1\n",
    "    thetas = np.zeros(n_params, np.float64)\n",
    "\n",
    "    for i in range (training_steps):\n",
    "        gradients = np.zeros(training_df.shape[1] - 1)\n",
    "        \n",
    "        for row in range (rows):\n",
    "            y = training_df.iloc[row][n_params]\n",
    "            \n",
    "            x_list = []\n",
    "            col = 0\n",
    "            while col < n_params:\n",
    "                x_list.append(training_df.iloc[row][col])\n",
    "                col += 1\n",
    "\n",
    "            x_numpy_array = np.asarray(x_list)\n",
    "            gradients = np.add(gradients, x_numpy_array * (y - sigmoid(np.matmul(x_numpy_array, thetas.T))))\n",
    "    \n",
    "\n",
    "                   \n",
    "        thetas = np.add(thetas, learning_rate * gradients)\n",
    "        \n",
    "    return thetas\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b67a70",
   "metadata": {},
   "source": [
    "The get_predicions function would look at all of the parametets for a given example and make a predicitons given the thetas we found in the previous step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ce0c413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(thetas, testing_df): \n",
    "    \n",
    "    n_tests = len(testing_df)\n",
    "    s = [1] * len(testing_df)\n",
    "    n_params = len(testing_df.columns) - 1\n",
    "    predictions = []\n",
    "    \n",
    "    test = 0\n",
    "    while test < n_tests:\n",
    "\n",
    "        x_vector = []\n",
    "        col = 0\n",
    "        while col < n_params:\n",
    "            x_vector.append(testing_df.iloc[test][col])\n",
    "            col += 1\n",
    "\n",
    "        prediction = sigmoid(dot(thetas, x_vector))\n",
    "\n",
    "        if prediction > 0.5:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "\n",
    "\n",
    "        test += 1\n",
    "\n",
    "    return predictions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfabf4d",
   "metadata": {},
   "source": [
    "I would use the check_validity function to see how accurate the predictions were by comparing my predicted outputs with the true outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "d0cf439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validity(predictions, testing_df):\n",
    "    \n",
    "#     file_names = {}\n",
    "#     file_names[\"./simple-test.csv\"] = \"Simple\"\n",
    "#     file_names[\"./netflix-test.csv\"] = \"Netflix\"\n",
    "#     file_names[\"ancestry-test.csv\"] = \"Ancestry\"\n",
    "#     file_names[\"./heart-test.csv\"] = \"Heart\"\n",
    "\n",
    "\n",
    "    n_tests = len(testing_df)\n",
    "    n_params = len(testing_df.columns) - 1\n",
    "    n_y0_tests = 0\n",
    "    n_y1_tests = 0\n",
    "\n",
    "    correct_predictions = 0\n",
    "    correct_0_predictions = 0\n",
    "    correct_1_predictions = 0\n",
    "    test = 0\n",
    "    while test < n_tests:\n",
    "        if (testing_df.iloc[test][n_params] == 0):\n",
    "            n_y0_tests += 1\n",
    "        else:\n",
    "            n_y1_tests += 1\n",
    "        if (predictions[test] == testing_df.iloc[test][n_params]):\n",
    "            correct_predictions += 1\n",
    "            if predictions[test] == 0:\n",
    "                correct_0_predictions += 1\n",
    "            else:\n",
    "                correct_1_predictions += 1\n",
    "\n",
    "        test += 1\n",
    "\n",
    "    model_accuracy = correct_predictions / n_tests\n",
    "#     print(model_accuracy)\n",
    "\n",
    "    print(f\"Using Logistic Regression\")\n",
    "\n",
    "    print(f\"Class 0: tested {n_y0_tests}, correctly classified {correct_0_predictions}\")\n",
    "    print(f\"Class 1: tested {n_y1_tests}, correctly classified {correct_1_predictions}\")\n",
    "    print(f\"Overall: tested {n_tests}, correctly classified {correct_predictions}\")\n",
    "    print(f\"Accuracy: {model_accuracy}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "3ac6dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_train_df = pd.read_csv('./simple-train.csv')\n",
    "s = [1] * len(simple_train_df)\n",
    "simple_train_df.insert(0, 'x0', s)\n",
    "training_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "877386fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_thetas = gradient_ascent(simple_train_df, training_steps, .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "f6b76a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14577434  0.82004294 -0.06660849]\n"
     ]
    }
   ],
   "source": [
    "print(simple_thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ca1b6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_test_df = pd.read_csv('./simple-test.csv')\n",
    "s = [1] * len(simple_test_df)\n",
    "simple_test_df.insert(0, 'x0', s)\n",
    "training_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "012905ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_predictions = get_predictions(simple_thetas, simple_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "1bc0d818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Logistic Regression\n",
      "Class 0: tested 2, correctly classified 2\n",
      "Class 1: tested 2, correctly classified 2\n",
      "Overall: tested 4, correctly classified 4\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "check_validity(simple_predictions, simple_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "0f1d40d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_train_df = pd.read_csv('./netflix-train.csv')\n",
    "del netflix_train_df['Demographic']\n",
    "s = [1] * len(netflix_train_df)\n",
    "netflix_train_df.insert(0, 'x0', s)\n",
    "training_steps = 3000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "c907fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_thetas = gradient_ascent(netflix_train_df, training_steps, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "32192410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.50497638  0.23104825 -0.0028295  -0.13271726 -0.09906566  0.2528209\n",
      "  0.00276221 -0.02652852  0.21079665 -0.01290325 -0.0928241   0.08403367\n",
      "  0.04436495  0.16957462 -0.07855384 -0.02945679 -0.02897179 -0.01414843\n",
      "  0.22249834  1.86511166]\n"
     ]
    }
   ],
   "source": [
    "print(netflix_thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "df21f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_test_df = pd.read_csv('./netflix-test.csv')\n",
    "del netflix_test_df['Demographic']\n",
    "s = [1] * len(netflix_test_df)\n",
    "netflix_test_df.insert(0, 'x0', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "7760bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_predictions = get_predictions(netflix_thetas, netflix_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "2a2d5e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Logistic Regression\n",
      "Class 0: tested 248, correctly classified 150\n",
      "Class 1: tested 252, correctly classified 189\n",
      "Overall: tested 500, correctly classified 339\n",
      "Accuracy: 0.678\n"
     ]
    }
   ],
   "source": [
    "check_validity(netflix_predictions, netflix_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "8e4ef2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelyhood when all paramters are 0: -3119.1623125199\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_rows = len(netflix_train_df)\n",
    "n_params = len(netflix_train_df.columns) - 1\n",
    "\n",
    "sum = 0\n",
    "\n",
    "for row in range (n_rows): \n",
    "    y = netflix_train_df.iloc[row][n_params]\n",
    "    sum += (y * np.log(0.5)) + ((1 - y) * (np.log(0.5)))\n",
    "    \n",
    "print(f\"Log Likelyhood when all paramters are 0: {sum}\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "fb8ae2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelyhood after training: -2601.775260916126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_rows = len(netflix_train_df)\n",
    "n_params = len(netflix_train_df.columns) - 1\n",
    "\n",
    "sum = 0\n",
    "\n",
    "for row in range(n_rows):\n",
    "    \n",
    "    y = netflix_train_df.iloc[row][n_params]\n",
    "    \n",
    "    x_list = []\n",
    "    col = 0\n",
    "    while col < n_params:\n",
    "        x_list.append(netflix_train_df.iloc[row][col])\n",
    "        col += 1\n",
    "\n",
    "    x_numpy_array = np.asarray(x_list)\n",
    "    \n",
    "    \n",
    "    sig_theta_x = sigmoid(np.matmul(x_numpy_array, netflix_thetas.T))\n",
    "    \n",
    "    sum += (y * np.log(sig_theta_x)) + ((1 - y) * np.log(1 - sig_theta_x))\n",
    "    \n",
    "print(f\"Log Likelyhood after training: {sum}\")\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6aec1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_train_df = pd.read_csv('./ancestry-train.csv')\n",
    "s = [1] * len(ancestry_train_df)\n",
    "ancestry_train_df.insert(0, 'x0', s)\n",
    "training_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e65a3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_test_df = pd.read_csv('./ancestry-test.csv')\n",
    "s = [1] * len(ancestry_test_df)\n",
    "ancestry_test_df.insert(0, 'x0', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "cf087c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = 10000\n",
    "ancestry_thetas = gradient_ascent(ancestry_train_df, training_steps, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "6cfffa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_predictions = get_predictions(ancestry_thetas, ancestry_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "bca0f576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Logistic Regression\n",
      "Class 0: tested 109, correctly classified 98\n",
      "Class 1: tested 75, correctly classified 56\n",
      "Overall: tested 184, correctly classified 154\n",
      "Accuracy: 0.8369565217391305\n"
     ]
    }
   ],
   "source": [
    "check_validity(ancestry_predictions, ancestry_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f2a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
